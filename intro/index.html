<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-fire.jpg">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-fire.jpg">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-fire.jpg">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"trl-0.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"flat"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="一个正在建设中的网页">
<meta property="og:url" content="https://trl-0.github.io/intro/index.html">
<meta property="og:site_name" content="一个正在建设中的网页">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="TRL-0">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://trl-0.github.io/intro/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>一个正在建设中的网页</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">一个正在建设中的网页</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>博客</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://trl-0.github.io/blog/Sampling-Method/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="TRL-0">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="一个正在建设中的网页">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/blog/Sampling-Method/" class="post-title-link" itemprop="url">Sampling Method</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-04-05 15:46:19 / 修改时间：21:14:18" itemprop="dateCreated datePublished" datetime="2023-04-05T15:46:19+08:00">2023-04-05</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Basic-Sampling-Algorithms（基础采样算法）"><a href="#Basic-Sampling-Algorithms（基础采样算法）" class="headerlink" title="Basic Sampling Algorithms（基础采样算法）"></a>Basic Sampling Algorithms（基础采样算法）</h2><p>（施工中…）</p>
<!-- ### Rejection Sampling（拒绝采样）

假设我们希望从概率分布$f(x)$中采样，但直接采样是困难的，同时我们去采样一个常见分布$g(x)$是简单的，因为对任意$x_i$有
$$
\frac{1}{k}f(x_i)=\frac{f(x_i)}{kg(x_i)}g(x_i):=pg(x_i)
$$
其中$k$满足$kg(x)\geq f(x)$恒成立且$k$尽可能小，因此此时我们可以将$p$看作是一个概率项，我们 -->
<h2 id="Markov-Chain-Monte-Carlo（MCMC，马尔科夫链蒙特卡洛）"><a href="#Markov-Chain-Monte-Carlo（MCMC，马尔科夫链蒙特卡洛）" class="headerlink" title="Markov Chain Monte Carlo（MCMC，马尔科夫链蒙特卡洛）"></a>Markov Chain Monte Carlo（MCMC，马尔科夫链蒙特卡洛）</h2><p>MCMC的作用是：可以帮我们从任意（无论有没有解析形式的）分布上抽样一批数据，然后用这堆抽样数据的均值作为对这个分布期望的估计。</p>
<h3 id="Markov-Chain（马尔科夫链）"><a href="#Markov-Chain（马尔科夫链）" class="headerlink" title="Markov Chain（马尔科夫链）"></a>Markov Chain（马尔科夫链）</h3><p>⼀阶马尔科夫链的定义：一系列随机变量$X_{1},X_{2},\dots X_{t},\dots$，满足某一时刻状态转移的概率只依赖于它的前一个状态，即：</p>
<script type="math/tex; mode=display">
p(X_{t+1}\mid X_{1}\dots X_{t})=p(X_{t+1}\mid X_{t}),\quad t=1,2,\dots</script><p>其中$X_i$表示在链上跳转第$i$步后所处的状态，$X_i$的取值范围即为马尔科夫链的状态数。状态数可以是有限的，也可以是无限的，因此可以用于连续概率分布和离散概率分布。</p>
<p>因为某一时刻状态转移只依赖于它的前一个状态，那么我们只要能求出系统中任意两个状态之间的转移概率，最终可得到状态转移概率矩阵$P$，$P_{ij}=p(X_{t+1}=j\mid X_t=i)$。</p>
<p>我们有这样的一个定理：如果一个非周期的，任意两个状态是连通的马尔科夫链有状态转移矩阵$P$，那么$P_{ij}^n$在$n\to \infty$时有极限，且与$i$无关，记为：</p>
<script type="math/tex; mode=display">
\lim_{n\to \infty}P_{ij}^n=\pi(j)</script><p>由于</p>
<script type="math/tex; mode=display">
p(X_{n+1}=j)=\sum_{i=0}^{\infty} p(X_n=i)P_{ij}</script><p>两边对$n$取极限可得</p>
<script type="math/tex; mode=display">
\pi(j)=\sum_{i=0}^{\infty} \pi(i)P_{ij}</script><p>我们还有个结论：$\pi$是方程$\pi P=\pi$的唯一非负解，$\pi=\left(\pi(1),\pi(2),\cdots\right)$，$\sum_{i=1}^{\infty}\pi(i)=1$，此时我们称$\pi$为该马尔科夫链的平稳分布（Stationary Distribution）。</p>
<p>对链上的任意时刻$t$，都存在概率分布$\pi_t(x)$，使得$X_t\sim \pi_t(x)$。由上述收敛定理，我们知道$\pi_t(x)$将收敛到平稳分布$\pi(x)$，即存在一个足够大的$T$，$X_T,X_{T+1},\cdots$都可以看做是平稳分布$\pi(x)$的样本。</p>
<p>因此，如果我们得到了某个平稳分布所对应的马尔科夫链状态转移矩阵，我们就很容易采用出这个平稳分布的样本集。首先，基于初始任意简单概率分布比如高斯分布$\pi_0(x)$采样得到状态值$x_0$，基于状态转移矩阵可采样状态值$x_1$，一直进行下去，当状态转移进行到一定的次数$T$时，我们认为此时的采样集$\{X_T,X_{T+1},\cdots\}$即是符合我们的平稳分布的对应样本集。</p>
<h3 id="Detailed-Balance-Condition（细致平稳条件）"><a href="#Detailed-Balance-Condition（细致平稳条件）" class="headerlink" title="Detailed Balance Condition（细致平稳条件）"></a>Detailed Balance Condition（细致平稳条件）</h3><p>马尔科夫链的细致平稳条件定义如下：如果非周期马尔科夫链状态转移矩阵$P$和概率分布 $\pi(x)$ 满足：</p>
<script type="math/tex; mode=display">
\pi(i)P(i,j)=\pi(j)P(j,i),\quad \forall \,i,j\\</script><p>则称概率分布 $\pi(x)$ 是状态转移矩阵 $P$ 的平稳分布。</p>
<h3 id="Markov-Chain-sampling（马尔科夫链采样）"><a href="#Markov-Chain-sampling（马尔科夫链采样）" class="headerlink" title="Markov Chain sampling（马尔科夫链采样）"></a>Markov Chain sampling（马尔科夫链采样）</h3><p>如果我们希望采样分布$f$，那我们找到它的状态转移矩阵$P$即可。</p>
<p>一般情况下，目标平稳分布$\pi(x)$和某个马尔科夫链的状态转移矩阵$Q$不满足细致平稳条件，即：</p>
<script type="math/tex; mode=display">
\pi(x_t)Q(x_t,x_{t+1})\neq\pi(x_{t+1})Q(x_{t+1},x_t)</script><p>因此引入$\alpha(x_t,x_{t+1})=\pi(x_{t+1})Q(x_{t+1},x_t)$，此时记$P(x_t,x_{t+1})=Q(x_t,x_{t+1})\alpha(x_t,x_{t+1})$，则有</p>
<script type="math/tex; mode=display">
\begin{gather*}
    \pi(x_t)Q(x_t,x_{t+1})\alpha(x_t,x_{t+1})=\pi(x_{t+1})Q(x_{t+1},x_t)\alpha(x_{t+1},x_t)\\\\
    \pi(x_t)P(x_t,x_{t+1})=\pi(x_{t+1})P(x_{t+1},x_t)
\end{gather*}</script><p>其中$\alpha(x_t,x_{t+1})$称为接受率，取值范围为$[0,1]$，可以将它看成概率。</p>
<p>通常我们取的$Q$为常见易采样的分布，因此我们可以用类似拒绝采样的方法，从$Q(x_t,x)$中采样得到样本值$\hat{x}$，从均匀分布$U(0,1)$中采样$u$，若$u&lt;\alpha(x_t,\hat{x})$，则接受采样$x_{t+1}=\hat{x}$，否则拒绝采样$x_{t+1}=x_t$，即有$\alpha(x_t,\hat{x})$的概率接受采样。</p>
<h3 id="Metropolis-Hastings-sampling（M-H采样）"><a href="#Metropolis-Hastings-sampling（M-H采样）" class="headerlink" title="Metropolis-Hastings sampling（M-H采样）"></a>Metropolis-Hastings sampling（M-H采样）</h3><p>M-H采样是一种通过对变形来加快收敛速度的MC采样方法。</p>
<p>我们对式子$\alpha(x_t,x_{t+1}),\alpha(x_{t+1},x_{t})$同时扩大至不超过1，即令$A(x_t,x_{t+1})=\min\{\frac{\alpha(x_t,x_{t+1})}{\alpha(x_{t+1},x_{t})},1\}$，有：</p>
<script type="math/tex; mode=display">
pi(x_t)Q(x_t,x_{t+1})A(x_t,x_{t+1})=\pi(x_{t+1})Q(x_{t+1},x_t)A(x_{t+1},x_t)</script><p>将$A(x_t,x_{t+1})$作为新的接受率进行采样即可。</p>
<h3 id="Gibbs-Sampling（吉布斯采样）"><a href="#Gibbs-Sampling（吉布斯采样）" class="headerlink" title="Gibbs Sampling（吉布斯采样）"></a>Gibbs Sampling（吉布斯采样）</h3><p>Gibbs采样是一种指定了$Q(i,j)$使得接受率$A(i,j)=1$恒成立的M-H采样的特殊情况，即可以做到不拒绝转移。但是Gibbs采样必须是采样向量。</p>
<p>如果变量$\boldsymbol{x}=\left(x^1,x^2,\cdots,x^n\right)$服从$\pi(\boldsymbol{x})$这一分布，则我们选择的$Q$为：</p>
<script type="math/tex; mode=display">
Q(\boldsymbol x_t,\boldsymbol x_{t+1})=\pi(x_{t+1}^k\mid \boldsymbol{x}_t^{-k}),\quad \boldsymbol{x}_t^{-k}:=\left(x_{t+1}^1,\cdots ,x_{t+1}^{k-1},x_t^{k+1}\cdots,x_t^n\right)</script><p>上面式子的意思是，从$t$到$t+1$这一步，只改变向量$\boldsymbol{x}$的第$k$个维度，其他的维度保持不变。因此</p>
<script type="math/tex; mode=display">
Q(\boldsymbol x_{t+1},\boldsymbol x_{t})=\pi(x_t^k\mid\boldsymbol{x}_t^{-k})</script><p>此时</p>
<script type="math/tex; mode=display">
\begin{align*}
    \alpha(\boldsymbol x_t,\boldsymbol x_{t+1})&=\pi(\boldsymbol x_{t+1})Q(\boldsymbol x_{t+1},\boldsymbol x_t)\\\\
    &=\pi( x^k_{t+1}\mid \boldsymbol x_{t}^{-k})\pi(\boldsymbol{x}_{t}^{-k}) \pi(x_t^k\mid\boldsymbol{x}_t^{-k})\\\\
    \alpha(\boldsymbol x_{t+1},\boldsymbol x_{t})&=\pi(\boldsymbol x_{t})Q(\boldsymbol x_{t},\boldsymbol x_{t+1})\\\\
    &=\pi( x^k_{t}\mid \boldsymbol x_{t}^{-k})\pi(\boldsymbol{x}_{t}^{-k}) \pi(x_{t+1}^k\mid\boldsymbol{x}_t^{-k})
\end{align*}</script><p>可得$\alpha(\boldsymbol x_t,\boldsymbol x_{t+1})=\alpha(\boldsymbol x_{t+1},\boldsymbol x_{t})$</p>
<p>则$A(x_t,x_{t+1})=A(x_{t+1},x_{t})=1$</p>
<p>因此每次M-H算法产生的新$\hat{\boldsymbol x}$我们可以直接接受为下一个样本$\boldsymbol x_{t+1}=\hat{\boldsymbol x}$</p>
<p>当然，因为选取的$Q$的特殊性，使用Gibbs采样的前提是需采样分布的条件分布是易采样的。</p>
<p><strong>Reference</strong></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/37121528">马尔可夫链蒙特卡罗算法（MCMC） - 知乎 (zhihu.com)</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/420214359">MCMC与贝叶斯推断简介：从入门到放弃 - 知乎 (zhihu.com)</a></p>
<p>PRML</p>

      
    </div>

    
    
    

    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://trl-0.github.io/blog/Gamma-function-and-Beta-function/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="TRL-0">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="一个正在建设中的网页">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/blog/Gamma-function-and-Beta-function/" class="post-title-link" itemprop="url">Gamma function and Beta function</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-04-03 23:31:27" itemprop="dateCreated datePublished" datetime="2023-04-03T23:31:27+08:00">2023-04-03</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-04-05 12:42:47" itemprop="dateModified" datetime="2023-04-05T12:42:47+08:00">2023-04-05</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/mathematics/" itemprop="url" rel="index"><span itemprop="name">mathematics</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Gamma函数"><a href="#Gamma函数" class="headerlink" title="Gamma函数"></a>Gamma函数</h2><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>Gamma函数也称为第二类欧拉积分，其定义式为</p>
<script type="math/tex; mode=display">
\Gamma(z)=\int_0^{+\infty}t^{z-1}\mathrm e^{-t}\,\mathrm dt, \quad\Re (z)>0</script><h3 id="性质"><a href="#性质" class="headerlink" title="性质"></a>性质</h3><p>$\Gamma$函数是阶乘函数在复平面 $\mathbb C$ 上的解析延拓。所以阶乘函数的一些性质同样适用于$\Gamma$函数，如：</p>
<script type="math/tex; mode=display">
\begin{align*} 
    \Gamma(z+1)=& z!,\quad z\in\mathbb N^*\\\\
    \Gamma(1)=&1\\\\
    \Gamma(z+1)=&z\Gamma(z) 
\end{align*}</script><p>上述性质很好证明，但$\Gamma$函数还有其他性质，如余元公式：</p>
<script type="math/tex; mode=display">
\Gamma(z)\Gamma(1-z)=\frac{\pi}{\sin(\pi z)},\quad \Re(z)\in(0, 1)</script><p>特别的，取$z=\frac{1}{2}$时，得：</p>
<script type="math/tex; mode=display">
\Gamma\left(\frac12\right)=\sqrt\pi</script><h3 id="性质证明"><a href="#性质证明" class="headerlink" title="性质证明"></a>性质证明</h3><p>（施工中…）</p>
<h2 id="Beta函数"><a href="#Beta函数" class="headerlink" title="Beta函数"></a>Beta函数</h2><h3 id="定义-1"><a href="#定义-1" class="headerlink" title="定义"></a>定义</h3><p>Beta函数也称为第一类欧拉积分，记作 $\mathrm B(p,q)$，其定义式为：</p>
<script type="math/tex; mode=display">
\mathrm B (p,q)=\int_0^1x^{p-1}(1-x)^{q-1}\,\mathrm dx,\quad \Re(p),\Re(q)>0</script><h3 id="性质-1"><a href="#性质-1" class="headerlink" title="性质"></a>性质</h3><p>在定义中，我们令$x=\sin^2 t$，即可得$\mathrm B$函数的三角定义式：</p>
<script type="math/tex; mode=display">
\mathrm B (p,q)=2\int_0^{\frac\pi2}(\sin\theta)^{2p-1}(\cos\theta)^{2q-1}\,\mathrm d\theta</script><p>若做代换$x=\frac{u}{1+u} $,即得到$\mathrm B$函数的另一广义积分表达式，即:</p>
<script type="math/tex; mode=display">
\mathrm B (p,q)=\int_0^\infty\frac{u^{p-1}}{(1+u)^{p+q}}\,\mathrm du</script><h3 id="mathrm-B-函数与-Gamma-函数的关系"><a href="#mathrm-B-函数与-Gamma-函数的关系" class="headerlink" title="$\mathrm B$函数与$\Gamma$函数的关系"></a>$\mathrm B$函数与$\Gamma$函数的关系</h3><p>两函数最直接的关系是</p>
<script type="math/tex; mode=display">
\mathrm B (p,q)=\frac{\Gamma(p)\Gamma(q)}{\Gamma(p+q)}</script><p>因此可看出$\mathrm B$函数两变量是对称的，即</p>
<script type="math/tex; mode=display">
\mathrm B (p,q)=\mathrm B (q,p)</script><p>类似$\Gamma$函数，$\mathrm B$函数也存在递推关系：</p>
<script type="math/tex; mode=display">
 \mathrm B (p,q+1)=\frac {q} {p}\mathrm B (p+1,q)=\frac{q}{p+q}\mathrm B (p,q)</script><h3 id="多元Beta函数"><a href="#多元Beta函数" class="headerlink" title="多元Beta函数"></a>多元Beta函数</h3><p>Beta函数可以被扩充为多元的形式：</p>
<script type="math/tex; mode=display">
\mathrm B(\boldsymbol{\alpha})=\mathrm B(\alpha_1,\alpha_2,\dots,\alpha_n)=\frac{\Gamma(\alpha_1)\Gamma(\alpha_2)\cdots\Gamma(\alpha_n)}{\Gamma(\alpha_1+\alpha_2+\cdots+\alpha_n)}</script><h3 id="性质证明-1"><a href="#性质证明-1" class="headerlink" title="性质证明"></a>性质证明</h3><p>（施工中…）</p>
<p><strong>Reference</strong></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/479594751">特殊函数（1）——Gamma函数和Beta函数 - 知乎 (zhihu.com)</a></p>

      
    </div>

    
    
    

    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://trl-0.github.io/blog/Conjugate-distributions/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="TRL-0">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="一个正在建设中的网页">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/blog/Conjugate-distributions/" class="post-title-link" itemprop="url">Conjugate distributions</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-04-03 22:09:57" itemprop="dateCreated datePublished" datetime="2023-04-03T22:09:57+08:00">2023-04-03</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-04-05 21:18:39" itemprop="dateModified" datetime="2023-04-05T21:18:39+08:00">2023-04-05</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Bayesian-statistics/" itemprop="url" rel="index"><span itemprop="name">Bayesian statistics</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Bayesian-theorem（贝叶斯定理）"><a href="#Bayesian-theorem（贝叶斯定理）" class="headerlink" title="Bayesian theorem（贝叶斯定理）"></a>Bayesian theorem（贝叶斯定理）</h2><p><img src="/blog/Conjugate-distributions/1.svg" alt="贝叶斯定理" title="贝叶斯定理"></p>
<ul>
<li>似然函数（Likelihood）: 关于统计模型中的参数$\theta$的函数，表示模型参数中的似然性。</li>
<li>先验分布（Prior）：在未看到观测数据的时候参数$\theta$的不确定性的概率分布。</li>
<li>后验分布（Posterior）：考虑和给出数据$x$后所得到的条件概率分布。</li>
</ul>
<p>在贝叶斯定理中，参数先有一个先验认知（先验分布），然后通过观察新数据，得到后验认知（后验分布）。在给定观测数据$x$后$p(x)$为常数，故$\text{Posterior}\propto\text{Likelihood}\cdot\text{Prior}$</p>
<h2 id="Conjugate-distributions（共轭分布）"><a href="#Conjugate-distributions（共轭分布）" class="headerlink" title="Conjugate distributions（共轭分布）"></a>Conjugate distributions（共轭分布）</h2><p><b><span style="color:red;">前置知识：</span></b><a href="/blog/Gamma-function-and-Beta-function/">Gamma函数和Beta函数</a></p>
<p>在贝叶斯统计中，如果后验分布与先验分布属于同一个概率分布族（probability distribution family），即分布形式相同，则先验分布与后验分布被称为共轭分布（conjugate distributions），而先验分布被称为该似然函数的共轭先验（conjugate prior）。因为后验分布和先验分布形式相近，只是参数有所不同，这意味着当我们获得新的观察数据时，我们就能直接通过参数更新，获得新的后验分布，此后验分布将会在下次新数据到来的时候成为新的先验分布。如此一来，我们更新后验分布就不需要通过大量的计算，十分方便。</p>
<h3 id="Binomial-distribution-and-Beta-distribution（二项分布和贝塔分布）"><a href="#Binomial-distribution-and-Beta-distribution（二项分布和贝塔分布）" class="headerlink" title="Binomial distribution and Beta distribution（二项分布和贝塔分布）"></a>Binomial distribution and Beta distribution（二项分布和贝塔分布）</h3><p>伯努利分布（Bernoulli distribution）：对于参数$\mu\in [0,1]$，随机变量$x\in \{0,1\}$，其概率质量函数（PMF）为：</p>
<script type="math/tex; mode=display">
f(x;\mu)=\mu^x(1-\mu)^{n-x}</script><p>二项分布（Binomial distribution）：对于参数$n\in \mathbb{N},\mu\in[0,1]$，随机变量$x$的取值范围为$x\in \mathbb{N}, x\leq n$，其概率质量函数（PMF）为：</p>
<script type="math/tex; mode=display">
f(x;n,\mu)={\binom {n}{x}} \mu^x(1-\mu)^{n-x}</script><p>若$x\sim B(n,\mu)$，则随机变量$x$的期望为：</p>
<script type="math/tex; mode=display">
\begin{align*}
    \mathrm E[x]&=\sum_{x=0}^{n}x\cdot {\binom {n}{x}}\mu^x(1-\mu)^{n-x}\\\\
    &=\sum_{x=1}^{n}n\cdot {\binom {n-1}{x-1}}\mu^x(1-\mu)^{n-x}\\\\
    &=n\mu\sum_{x=1}^{n}  {\binom {n-1}{x-1}} \mu^{x-1}(1-\mu)^{n-x}=n\mu
\end{align*}</script><p>贝塔分布（Beta distribution）：对于参数$\alpha&gt;0,\beta&gt;0$，随机变量$x$的取值范围为$[0,1]$，其概率密度函数（PDF）为：</p>
<script type="math/tex; mode=display">
f(x;\alpha,\beta)=\frac{1}{\mathrm B(\alpha,\beta)}x^{\alpha-1}(1-x)^{\beta-1}</script><p>若$x\sim \mathrm{Beta} (\alpha,\beta)$，则随机变量$x$的期望为：</p>
<script type="math/tex; mode=display">
\begin{align*}
    \mathrm E[x]&=\int_0^1 x\cdot \frac{x^{\alpha-1}(1-x)^{\beta-1}}{\mathrm B(\alpha,\beta)} \,\mathrm dx\\\\
    &=\frac{1}{\mathrm B(\alpha,\beta)}\int_0^1x^\alpha(1-x)^{\beta-1}\,\mathrm dx\\\\
    &=\frac{\mathrm B(\alpha+1,\beta)}{\mathrm B(\alpha,\beta)}\int_0^1\frac{x^\alpha(1-x)^{\beta-1}}{\mathrm B(\alpha+1,\beta)}\,\mathrm dx\\\\
    &=\frac{\mathrm B(\alpha+1,\beta)}{\mathrm B(\alpha,\beta)}=\frac{\alpha}{\alpha+\beta}
\end{align*}</script><p>如果我们要估计二项分布的参数$\mu$，从Bayesian的观点来看，我们引入关于$\mu$的先验分布$p(\mu)$，若把先验分布选择为Beta分布，即</p>
<script type="math/tex; mode=display">
p(\mu;\alpha,\beta)=\frac{1}{\mathrm B(\alpha,\beta)}\mu^{\alpha-1}(1-\mu)^{\beta-1}</script><p>在观察到数据$x$后的似然函数$p(x\mid \mu)$的形式为：</p>
<script type="math/tex; mode=display">
p(x\mid \mu;n)={\binom {n}{x}} \mu^x(1-\mu)^{n-x}</script><p>则后验概率$p(\mu\mid x)$为：</p>
<script type="math/tex; mode=display">
\begin{align*}
    p(\mu\mid x;n,\alpha,\beta) &\propto p(x\mid \mu;n)p(\mu;\alpha,\beta)\\\\
    &\propto \mu^x(1-\mu)^{n-x}\cdot \mu^{\alpha-1}(1-\mu)^{\beta-1}\\\\
    &=\mu^{x+\alpha-1}(1-\mu)^{n-x+\beta-1}
\end{align*}</script><p>此时我们看到后验分布形式与先验分布相同，即</p>
<script type="math/tex; mode=display">
p(\mu\mid x;n,\alpha,\beta) \sim \mathrm{Beta}(x+\alpha,n-x+\beta)</script><h3 id="Multinomial-distribution-and-Dirichlet-distribution（多项分布和狄利克雷分布）"><a href="#Multinomial-distribution-and-Dirichlet-distribution（多项分布和狄利克雷分布）" class="headerlink" title="Multinomial distribution and Dirichlet distribution（多项分布和狄利克雷分布）"></a>Multinomial distribution and Dirichlet distribution（多项分布和狄利克雷分布）</h3><p>标准n-单纯形定义为：</p>
<script type="math/tex; mode=display">
 \Delta^n:=\left\{(t_{0},\dots ,t_{n})\in \mathbb {R} ^{n+1}~{\Bigg |}~\sum _{i=0}^{n}t_{i}=1, \,t_{i}\geq 0, \quad i=0,\ldots ,n\right\}</script><p>分类分布（Categorical distribution）：对于参数$\boldsymbol{\mu}=\{\mu_1,\dots,\mu_k\}\in \Delta^{k-1}$，随机变量$\boldsymbol{x}=\{x_1,\dots,x_k\}\in \mathbb{\{0,1\} }^k$，$\sum _{i=1}^{k}x_{i}=1$，其概率质量函数（PMF）为：</p>
<script type="math/tex; mode=display">
f(x_1,\dots,x_k;\mu_1,\dots,\mu_k) = \prod_{i=1}^{k}{\mu_i^{x_i}}</script><p>多项分布（Multinomial distribution）：对于参数$n\in \mathbb{N},\boldsymbol{\mu}=\{\mu_1,\dots,\mu_k\}\in \Delta^{k-1}$，随机变量$\boldsymbol{x}=\{x_1,\dots,x_k\}\in \mathbb{N}^k$，$\sum _{i=1}^{k}x_{i}=n$，其概率质量函数（PMF）为：</p>
<script type="math/tex; mode=display">
f(x_1,\dots,x_k;n,\mu_1,\dots,\mu_k)={n! \over x_{1}!\cdots x_{k}!}\prod_{i=1}^k\mu_{i}^{x_{i}}</script><p>若$\boldsymbol{x}\sim \mathrm{Multi}(n,\boldsymbol{\mu})$，则类似二项分布，可求得随机变量$\boldsymbol{x}$的期望为：</p>
<script type="math/tex; mode=display">
\mathrm E[\boldsymbol{x}]=\left(n\mu_1,n\mu_2,\cdots,n\mu_k\right)</script><p>狄利克雷分布（Dirichlet distribution）：对于参数$\boldsymbol{\alpha}=\{\alpha_1,\dots,\alpha_k\}&gt;\boldsymbol{0}$，随机变量$\boldsymbol{x}=\{x_1,\dots,x_k\}\in \Delta^{k-1}$，其概率密度函数（PDF）为：</p>
<script type="math/tex; mode=display">
f(x_1,\dots,x_k;\alpha_1,\dots,\alpha_k)=\frac{1}{\mathrm B(\boldsymbol{\alpha})}\prod_{i=1}^kx_i^{\alpha_i-1}</script><p>若$\boldsymbol{x}\sim \mathrm{Dir} (\boldsymbol{\alpha})$，则类似Beta分布，可求得随机变量$\boldsymbol{x}$的期望为：</p>
<script type="math/tex; mode=display">
\mathrm E[\boldsymbol{x}]=\left(\frac{\alpha_1}{\alpha_0},\frac{\alpha_2}{\alpha_0},\cdots,\frac{\alpha_k}{\alpha_0}\right),\quad   \alpha_0=\sum_{i=1}^k\alpha_i</script><p>如果我们要估计多项分布的参数$\boldsymbol{\mu}$，同样从Bayesian的观点来看，我们引入关于$\boldsymbol{\mu}$的先验分布$p(\boldsymbol{\mu})$，若把先验分布选择为Dirichlet分布，即</p>
<script type="math/tex; mode=display">
p(\boldsymbol{\mu};\boldsymbol{\alpha})=\frac{1}{\mathrm B(\boldsymbol{\alpha})}\prod_{i=1}^k\mu_i^{\alpha_i-1}</script><p>在观察到数据$\boldsymbol{x}$后的似然函数$p(\boldsymbol{x}\mid \boldsymbol{\mu})$的形式为：</p>
<script type="math/tex; mode=display">
p(\boldsymbol{x}\mid \boldsymbol{\mu};n)={n! \over x_{1}!\cdots x_{k}!}\prod_{i=1}^k\mu_{i}^{x_{i}}</script><p>则后验概率$p(\boldsymbol\mu\mid \boldsymbol x)$为：</p>
<script type="math/tex; mode=display">
\begin{align*}
    p(\boldsymbol \mu\mid \boldsymbol x;n,\boldsymbol\alpha) &\propto p(\boldsymbol x\mid \boldsymbol\mu;n)p(\boldsymbol\mu;\boldsymbol\alpha)\\\\
    &\propto \prod_{i=1}^k\mu_{i}^{x_{i}}\cdot \prod_{i=1}^k\mu_i^{\alpha_i-1}\\\\
    &=\prod_{i=1}^k\mu_i^{x_i+\alpha_i-1}
\end{align*}</script><p>此时我们看到后验分布形式与先验分布相同，即</p>
<script type="math/tex; mode=display">
\begin{gather*}
    p(\boldsymbol\mu\mid \boldsymbol x;n,\boldsymbol\alpha) \sim \mathrm{Dir}(\boldsymbol{\alpha}^\prime),\\\\
    \boldsymbol \alpha^\prime=\left(x_1+\alpha_1-1,\cdots,x_k+\alpha_k-1\right)
\end{gather*}</script><p><strong>Reference</strong></p>
<p><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Conjugate_prior">Conjugate prior - Wikipedia</a></p>
<p><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Dirichlet_distribution">Dirichlet distribution - Wikipedia</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/103854460">“共轭分布”是什么？ - 知乎 (zhihu.com)</a></p>

      
    </div>

    
    
    

    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://trl-0.github.io/blog/Latent-Dirichlet-Allocation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="TRL-0">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="一个正在建设中的网页">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/blog/Latent-Dirichlet-Allocation/" class="post-title-link" itemprop="url">Latent Dirichlet Allocation</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-04-03 17:07:12" itemprop="dateCreated datePublished" datetime="2023-04-03T17:07:12+08:00">2023-04-03</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-04-04 00:01:06" itemprop="dateModified" datetime="2023-04-04T00:01:06+08:00">2023-04-04</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>前置知识：</p>
<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2>
      
    </div>

    
    
    

    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://trl-0.github.io/blog/Probabilistic-Graphical-Models/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="TRL-0">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="一个正在建设中的网页">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/blog/Probabilistic-Graphical-Models/" class="post-title-link" itemprop="url">Probabilistic Graphical Models</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-04-03 12:57:55 / 修改时间：13:01:24" itemprop="dateCreated datePublished" datetime="2023-04-03T12:57:55+08:00">2023-04-03</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>（施工中…）</p>
<h2 id="Bayesian-networks"><a href="#Bayesian-networks" class="headerlink" title="Bayesian networks"></a>Bayesian networks</h2><script type="math/tex; mode=display">
p(x_1,x_2,\dots, x_n)=\prod_{i\in V}p(x_i\mid \boldsymbol{x}_{Pa(i)})</script><p>（施工中…）</p>

      
    </div>

    
    
    

    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://trl-0.github.io/blog/Gaussian-Mixture-Model/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="TRL-0">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="一个正在建设中的网页">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/blog/Gaussian-Mixture-Model/" class="post-title-link" itemprop="url">Gaussian Mixture Model</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-04-02 21:46:46" itemprop="dateCreated datePublished" datetime="2023-04-02T21:46:46+08:00">2023-04-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-04-04 00:00:53" itemprop="dateModified" datetime="2023-04-04T00:00:53+08:00">2023-04-04</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>GMM算法是一种基于概率统计的聚类算法，其核心思想是将数据集中的数据看成是由多个高斯分布混合而成的。这些高斯分布的参数是未知的，需要通过EM算法来进行估计。具体而言，GMM算法首先随机初始化高斯分布的参数，然后通过EM算法进行迭代优化，直到收敛为止。在E步中，计算每个数据点属于每个高斯分布的概率；在M步中，根据这些概率更新每个高斯分布的参数。通过迭代优化，GMM算法可以自动地确定最优的高斯分布个数和参数，将数据集划分为不同的聚类簇。GMM算法的优点在于能够处理复杂的数据分布，且对噪声和异常点的影响相对较小。但是，GMM算法的计算量较大，且需要预先设定高斯分布的个数。</p>
<h2 id="GMM"><a href="#GMM" class="headerlink" title="GMM"></a>GMM</h2><p>（施工中…）</p>

      
    </div>

    
    
    

    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://trl-0.github.io/blog/Probabilistic-Latent-Semantic-Analysis/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="TRL-0">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="一个正在建设中的网页">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/blog/Probabilistic-Latent-Semantic-Analysis/" class="post-title-link" itemprop="url">Probabilistic Latent Semantic Analysis</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-04-02 20:35:01" itemprop="dateCreated datePublished" datetime="2023-04-02T20:35:01+08:00">2023-04-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-04-04 00:02:33" itemprop="dateModified" datetime="2023-04-04T00:02:33+08:00">2023-04-04</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>概率潜在语义分析（Probabilistic Latent Semantic Analysis, PLSA）是一种用于文本处理、信息检索和主题分析的概率模型。其在潜在语义分析LSA的基础上，通过引入概率图模型为基础，对文档和主题进行建模。</p>
<p>它假设一篇文档中的每个词都是从一组概率分布中随机生成的，这组分布被称为“主题”。在PLSA模型中，每个文档都被表示为一组主题的概率分布，而每个主题也被表示为一组单词的概率分布。因此，PLSA模型可以将文档和单词映射到低维主题空间中，这有助于提取文本的隐藏语义信息。</p>
<p>PLSA模型的训练过程涉及到最大化似然函数，通常使用EM算法来实现。在使用PLSA进行文本分类、信息检索、主题建模等任务时，可以将文档表示为主题的概率分布，从而对文本进行分类、匹配等操作。</p>
<h2 id="LSA（潜在语义分析）"><a href="#LSA（潜在语义分析）" class="headerlink" title="LSA（潜在语义分析）"></a>LSA（潜在语义分析）</h2><p>潜在语义分析（Latent Semantic Analysis, LSA）基于词项在文档集合中的共现特性，表达词项的潜在意义。它通过“降维”的方式获得主题。</p>
<p>假设我们已经给出了一个文本文档集合$D = \{d_1,\dots, d_M\}$与词汇表$W = \{w_1，\dots, w_N\}$，其中$M$表示文档数，$N$表示词项数，对每篇文档的单词词频进行统计并建立词项文档矩阵（term-document matrix）$C\in \mathbb{R}^{N\times M}$。由于文档分布在稀疏词汇空间上，这种文档表示难以分析利用（即$C$往往是稀疏矩阵），因此通过降维让文档分布在一个维度有限的潜在语义空间上。LSA利用矩阵低秩近似方式将词项信息映射到潜在语义空间 (主题空间)。对$C$进行截断的奇异值分解（truncated SVD），截断所用的$K$则为主题数量。</p>
<script type="math/tex; mode=display">
C=U\Sigma V^T,\quad C\approx \hat{U}\hat{\Sigma} \hat{V}^T (\text{truncated})</script><p>其中$\hat{U}\in \mathbb{R}^{N \times K}$是词项特征向量组成的矩阵，表示词项在隐含语义空间（主题空间）中的变换，$CC^T$表示词项相似度，值为两个词项在文档中共现次数；$\hat{V}\in \mathbb{R}^{M \times K}$是文档特征向量组成的矩阵，表示文档在隐含语义空间（主题空间）中的变换，$C^TC$表示文档相似度，值为两个文档中共同词项的数量。通过奇异值分解我们即完成了对词和文档的分类，同时得到了不同主题的文章其相关的高频词。</p>
<p>LSA的优势是可以一定程度建模一义多词现象，同时可以得到词项相似度、文档相似度、词项与文档的相似度，可用于信息检索。但其仍有缺陷，潜在语义空间中的元素缺少解释；$K$是经验性的设定；SVD求解复杂度较高，存在数值不稳定问题，当有新的文档来到时，若要更新模型需重新训练。</p>
<h2 id="PLSA（概率潜在语义分析）"><a href="#PLSA（概率潜在语义分析）" class="headerlink" title="PLSA（概率潜在语义分析）"></a>PLSA（概率潜在语义分析）</h2><p><b><span style="color:red;">前置知识：</span></b><a href="/blog/EM-algorithm/">EM算法</a>，<a href="/blog/Probabilistic-Graphical-Models/">概率图模型</a></p>
<p>PLSA加入了主题这一隐变量，记主题集合为$Z = \{z_1,\dots, z_K\}$。主题模型主要关心文档生成主题，主题生成词汇。PLSA利用概率方法学习隐含的主题分布，是一种基于有向图的生成模型：<br><img src="/blog/Probabilistic-Latent-Semantic-Analysis/1.png" alt="PLSA概率图模型" title="PLSA概率图模型"></p>
<p>上述变量有两个状态：observed ($d,w$), latent ($z$)。由概率图的定义，联合概率$p(d_i,w_j,z_k)=p(d_i)p(z_k\mid d_i)p(w_j\mid z_k)$。其中$p(d_i)$表示生成文档$d_i$的概率；$p(z_k\mid d_i)$表示文档$d_i$生成主题$z_k$的概率（每个文档$d_i$的主题概率分布）；$p(w_j\mid z_k)$表示主题$z_k$生成词项$w_j$的概率（每个主题$z_k$的词项概率分布）。</p>
<p>我们需要对观测到的变量$d, w$做最大似然估计，得到对数似然函数：</p>
<script type="math/tex; mode=display">
\begin{align*}
    L &=\log \left(\prod_{i=1}^M\prod_{j=1}^Np(d_i,w_j)^{n(d_i,w_j)}\right)\\\\
    &=\sum_{i=1}^M\sum_{j=1}^Nn(d_i,w_j)\log p(d_i,w_j)\\\\
    &=\sum_{i=1}^M\sum_{j=1}^Nn(d_i,w_j)\log \left(\sum_{k=1}^Kp(d_i,w_j,z_k)\right)\\\\
    &=\sum_{i=1}^M\sum_{j=1}^Nn(d_i,w_j)\log \left(\sum_{k=1}^Kp(d_i)p(z_k\mid d_i)p(w_j\mid z_k)\right)\\\\
    &\propto \sum_{i=1}^M\sum_{j=1}^Nn(d_i,w_j)\log \left(\sum_{k=1}^Kp(z_k\mid d_i)p(w_j\mid z_k)\right)
\end{align*}</script><p>其中$n(d_i,w_j)$表示文档$d_i$中词项$w_j$出现的次数。</p>
<p>在已知数据的情况下，$p(d_i)=\frac{1}{M}$为一常数，因此我们要优化的参数即为$p(z_k\mid d_i), p(w_j\mid z_k)$，约有$(M+N)K$个。由于对数里求和的形式不容易直接优化，且含有隐变量，因此考虑使用EM算法求解。</p>
<p>在E-step中，我们假设模型参数给定，即$\hat{p}(z_k\mid d_i), \hat{p}(w_j\mid z_k)$已知，可求解后验概率$\hat{p}(z_k\mid d_i,w_j)$：</p>
<script type="math/tex; mode=display">
\begin{align*}
    \hat{p}(z_k\mid d_i,w_j)&=\frac{\hat{p}(d_i,w_j,z_k)}{\hat{p}(d_i,w_j)}\\\\
    &=\frac{\hat{p}(z_k\mid d_i)\hat{p}(w_j\mid z_k)}{\sum_{k^{\prime}}^K \hat{p}(z_{k^{\prime}}\mid d_i)\hat{p}(w_j\mid z_{k^{\prime}})}
\end{align*}</script><p>得到当前需优化的Q函数：</p>
<script type="math/tex; mode=display">
Q=\sum_{i=1}^M\sum_{j=1}^Nn(d_i,w_j)\sum_{k=1}^K \hat{p}(z_k\mid d_i,w_j)\log \left(p(z_k\mid d_i)p(w_j\mid z_k)\right)</script><p>在M-step中，我们则需优化参数$p(z_k\mid d_i), p(w_j\mid z_k)$来最大化Q函数。这是个受限制的优化，需要各参数满足：</p>
<script type="math/tex; mode=display">
\begin{gather*}
     \sum_{k=1}^Kp(z_k\mid d_i)=1,\quad i=1,2,\dots,M\\
     \sum_{j=1}^Np(w_j\mid z_k)=1,\quad k=1,2,\dots,K
\end{gather*}</script><p>因此引入拉格朗日乘子$\boldsymbol{\lambda}=\{\lambda_1,\dots,\lambda_M\}, \boldsymbol{\mu}=\{\mu_1,\dots,\mu_K\}$，将优化目标修改为:</p>
<script type="math/tex; mode=display">
\mathcal{H}=Q+\sum_{i=1}^M\lambda_i\left(1-\sum_{k=1}^Kp(z_k\mid d_i)\right)+\sum_{k=1}^K\mu_k\left(1-\sum_{j=1}^Np(w_j\mid z_k)\right)</script><p>对各参数求导并令导数为0，得到:</p>
<script type="math/tex; mode=display">
\begin{gather*}
    \sum_{j=1}^{N}n(d_i,w_j)\hat{p}(z_k\mid d_i,w_j)-\lambda_i p(z_k\mid d_i)=0, \quad \forall\, k,i\\\\
    \sum_{i=1}^Mn(d_i,w_j)\hat{p}(z_k\mid d_i,w_j)-\mu_k p(w_j\mid z_k)=0,\quad \forall\, j,k
\end{gather*}</script><p>利用概率归一化条件，上述两个式子分别对$k,j$求和，得到：</p>
<script type="math/tex; mode=display">
\begin{gather*}
\lambda_i=\sum_{k=1}^K\sum_{j=1}^{N}n(d_i,w_j)\hat{p}(z_k\mid d_i,w_j),\quad \forall \,i\\\\
\mu_k=\sum_{j=1}^N\sum_{i=1}^Mn(d_i,w_j)\hat{p}(z_k\mid d_i,w_j),\quad \forall \, k
\end{gather*}</script><p>整理可得极大化Q函数时各个模型参数的值：</p>
<script type="math/tex; mode=display">
\begin{align*}
    p(z_k\mid d_i)&=\frac{\sum_{j=1}^{N}n(d_i,w_j)\hat{p}(z_k\mid d_i,w_j)}{\sum_{k^{\prime}=1}^K\sum_{j=1}^{N}n(d_i,w_j)\hat{p}(z_{k^{\prime}}\mid d_i,w_j)}\\\\
    &=\frac{\sum_{j=1}^{N}n(d_i,w_j)\hat{p}(z_k\mid d_i,w_j)}{n(d_i)}\\\\
    p(w_j\mid z_k)&=\frac{\sum_{i=1}^Mn(d_i,w_j)\hat{p}(z_k\mid d_i,w_j)}{\sum_{j^{\prime}=1}^N\sum_{i=1}^Mn(d_i,w_{j^{\prime}})\hat{p}(z_k\mid d_i,w_{j^{\prime}})}
\end{align*}</script><p>其中$n(d_i)=\sum_{j=1}^Nn(d_i,w_j)$，表示文档$d_i$中的词项总数。</p>
<p>总结PLSA的流程：</p>
<ul>
<li>设置参数$p(z_k\mid d_i), p(w_j\mid z_k)$的初始值；</li>
<li>迭代执行E-step, M-step直至收敛或达到最大迭代轮次，得到参数$p(z_k\mid d_i), p(w_j\mid z_k)$的终值。</li>
</ul>
<p>PLSA是LSA的概率化版本，主题以词的概率分布表示，文档则以主题的概率分布表示，相比LSA，概率化的理论解释性更好。但主题数量仍然需要经验性确定；需要学习的参数量多，有过拟合的可能性，需要大量的样本才能得到较好的模型；没有解决为未知文档分配主题概率的问题。</p>
<p><strong>Reference</strong></p>
<p>上课的PPT</p>
<p><a target="_blank" rel="noopener" href="https://link.springer.com/article/10.1023/A:1007617005950">Unsupervised Learning by Probabilistic Latent Semantic Analysis | SpringerLink</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/401513907">【学习笔记】pLSA算法原理手写推导 - 知乎 (zhihu.com)</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/31235789">pLSA原理及其代码实现 - 知乎 (zhihu.com)</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/54168587">情感分析中的PLSA、LDA模型 - 知乎 (zhihu.com)</a></p>

      
    </div>

    
    
    

    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://trl-0.github.io/blog/EM-Algorithm/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="TRL-0">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="一个正在建设中的网页">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/blog/EM-Algorithm/" class="post-title-link" itemprop="url">EM Algorithm</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-04-02 16:39:45" itemprop="dateCreated datePublished" datetime="2023-04-02T16:39:45+08:00">2023-04-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-04-05 15:36:54" itemprop="dateModified" datetime="2023-04-05T15:36:54+08:00">2023-04-05</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>期望最大化算法（expectation-maximization algorithm, EM algorithm）是最常见的隐变量估计方法，目的是求解带有隐变量的最大似然问题。EM算法是一种迭代优化策略，它的计算方法中每一次迭代都分两步，其中一个为期望步（E步），另一个为极大步（M步）。在E步中，算法假设已经有了一组参数估计，然后根据这组参数估计，计算出隐含变量的期望值。在M步中，算法根据E步计算出的隐含变量的期望值，通过最大化对数似然函数的方式来更新参数估计。通过交替执行E步和M步，EM算法可以逐步优化模型参数，直到达到收敛条件为止。虽然EM算法不一定能够得到全局最优解，但是在许多实际问题中，它通常能够得到较好的局部最优解。</p>
<h2 id="EM算法推导"><a href="#EM算法推导" class="headerlink" title="EM算法推导"></a>EM算法推导</h2><p>我们记$\boldsymbol{X}$为所有观测到的数据，$\boldsymbol{Z}$为所有的隐变量，其对数似然函数为：</p>
<script type="math/tex; mode=display">
% \begin{equation}
    L(\theta)=\log p(\boldsymbol{X};\theta) =\log \sum_{z \in \boldsymbol{Z}} p(\boldsymbol{X}, z;\theta)
% \end{equation}</script><p>我们的目的是求出各参数让似然函数值达到最大，但上式中存在对数里求和的形式不方便求解，我们引入一个新的与隐变量有关的分布$q(z)$， $\sum_{z\in \boldsymbol{Z} }q(z)=1$：</p>
<script type="math/tex; mode=display">
% \begin{equation}
    L(\theta)=\log \sum_{z \in \boldsymbol{Z}} q(z)\frac{p(\boldsymbol{X}, z;\theta)}{q(z)}\geq \sum_{z \in \boldsymbol{Z}} q(z) \log \frac{p(\boldsymbol{X}, z;\theta)}{q(z)}
% \end{equation}</script><p>上述不等式可由Jensen不等式导出，当且仅当存在一个常数$c$，使得对$\forall z \in Z$，有：</p>
<script type="math/tex; mode=display">
\frac{p(X,z;\theta)}{q(z)}=c</script><p>由</p>
<script type="math/tex; mode=display">
\sum_{z\in \boldsymbol{Z}} q(z)=\frac{1}{c}\sum_{z\in \boldsymbol{Z}}p(\boldsymbol{X},z;\theta)=\frac{1}{c}p(\boldsymbol{X};\theta)=1</script><p>得$\forall z \in Z$:</p>
<script type="math/tex; mode=display">
q(z)=\frac{p(X,z;\theta)}{c}=\frac{p(X,z;\theta)}{p(\boldsymbol{X};\theta)}=p(z\mid \boldsymbol{X};\theta)</script><p>由此我们可以得到$L(\theta)$的一个下界：</p>
<script type="math/tex; mode=display">
\sum_{z \in \boldsymbol{Z}} p(z\mid \boldsymbol{X};\theta) \log \frac{p(\boldsymbol{X}, z;\theta)}{p(z\mid \boldsymbol{X};\theta)}
=\mathrm E_{\boldsymbol{Z}|\boldsymbol{X};\theta}[\log p(\boldsymbol{X}, z;\theta)]+H(p(\boldsymbol{Z}|\boldsymbol{X};\theta))</script><p>如果我们能极大化这个下界，则也在尝试极大化我们的对数似然。若给定模型参数$\theta=\hat{\theta}$，则后验概率$p(z|\boldsymbol{X};\hat{\theta})$可求，$H(p(\boldsymbol{Z}|\boldsymbol{X};\hat{\theta}))$为一常数，此时我们只需要极大化条件期望$\mathrm E_{\boldsymbol{Z}|\boldsymbol{X};\hat{\theta}}[\log p(\boldsymbol{X}, z;\theta)]$，更新$\theta$。通过交替更新的思路，不断迭代重复上述过程，以此来优化这个问题。</p>
<h2 id="EM算法流程"><a href="#EM算法流程" class="headerlink" title="EM算法流程"></a>EM算法流程</h2><ul>
<li>随机初始化模型参数$\theta$的初值$\theta^{(0)}$</li>
<li>迭代轮数$t$小于等于最大迭代轮数$T$时：<ul>
<li>E-step: 求解后验概率$p(z|\boldsymbol{X};\theta^{t})$，并得到当前需优化期望表达式（Q函数）<script type="math/tex">Q(\theta;\theta^{(t)})=\mathrm E_{\boldsymbol{Z}|\boldsymbol{X};\theta^{t}}[\log p(\boldsymbol{X}, z;\theta)]</script></li>
<li>M-step: 极大化$Q(\theta;\theta^{(t)})$，得到$\theta^{t+1}<script type="math/tex">$\theta^{t+1}=\mathop{\arg\max}\limits_{\theta}  Q(\theta;\theta^{(t)})</script></li>
<li>判断$\theta^{t+1}$是否收敛，若收敛则终止算法，否则继续交替地进行两个步骤。</li>
</ul>
</li>
</ul>
<h2 id="EM算法收敛性证明"><a href="#EM算法收敛性证明" class="headerlink" title="EM算法收敛性证明"></a>EM算法收敛性证明</h2><p>（施工中…）</p>
<p><strong>Reference</strong></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/pinard/p/6912636.html">EM算法原理总结 - 刘建平Pinard - 博客园 (cnblogs.com)</a></p>

      
    </div>

    
    
    

    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">TRL-0</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">8</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">TRL-0</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
