<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Gamma function and Beta function</title>
      <link href="/blog/Gamma-function-and-Beta-function/"/>
      <url>/blog/Gamma-function-and-Beta-function/</url>
      
        <content type="html"><![CDATA[<h2 id="Gamma-函数"><a href="#Gamma-函数" class="headerlink" title="$\Gamma$函数"></a>$\Gamma$函数</h2><p>（施工中…）</p>]]></content>
      
      
      <categories>
          
          <category> mathematics </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Conjugate distributions</title>
      <link href="/blog/Conjugate-distributions/"/>
      <url>/blog/Conjugate-distributions/</url>
      
        <content type="html"><![CDATA[<h2 id="Bayesian-theorem（贝叶斯定理）"><a href="#Bayesian-theorem（贝叶斯定理）" class="headerlink" title="Bayesian theorem（贝叶斯定理）"></a>Bayesian theorem（贝叶斯定理）</h2><p><img src="/blog/Conjugate-distributions/1.svg" alt="贝叶斯定理" title="贝叶斯定理"></p><ul><li>似然函数（Likelihood）: 关于统计模型中的参数$\theta$的函数，表示模型参数中的似然性。</li><li>先验分布（Prior）：在未看到观测数据的时候参数$\theta$的不确定性的概率分布。</li><li>后验分布（Posterior）：考虑和给出数据$x$后所得到的条件概率分布。</li></ul><p>在贝叶斯定理中，参数先有一个先验认知（先验分布），然后通过观察新数据，得到后验认知（后验分布）。在给定观测数据$x$后$p(x)$为常数，故$\text{Posterior}\propto\text{Likelihood}\cdot\text{Prior}$</p><h2 id="Conjugate-distributions（共轭分布）"><a href="#Conjugate-distributions（共轭分布）" class="headerlink" title="Conjugate distributions（共轭分布）"></a>Conjugate distributions（共轭分布）</h2><p><b><span style="color:red;">前置知识：</span></b><a href="/blog/Gamma-function-and-Beta-function/">Gamma函数和Beta函数</a></p><p>在贝叶斯统计中，如果后验分布与先验分布属于同一个概率分布族（probability distribution family），即分布形式相同，则先验分布与后验分布被称为共轭分布（conjugate distributions），而先验分布被称为似然函数的共轭先验（conjugate prior）。因为后验分布和先验分布形式相近，只是参数有所不同，这意味着当我们获得新的观察数据时，我们就能直接通过参数更新，获得新的后验分布，此后验分布将会在下次新数据到来的时候成为新的先验分布。如此一来，我们更新后验分布就不需要通过大量的计算，十分方便。</p><p>（施工中…）</p><p><strong>Reference</strong></p><p><a href="https://en.wikipedia.org/wiki/Conjugate_prior">Conjugate prior - Wikipedia</a></p><p><a href="https://zhuanlan.zhihu.com/p/103854460">“共轭分布”是什么？ - 知乎 (zhihu.com)</a></p>]]></content>
      
      
      <categories>
          
          <category> Bayesian statistics </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Latent Dirichlet Allocation</title>
      <link href="/blog/Latent-Dirichlet-Allocation/"/>
      <url>/blog/Latent-Dirichlet-Allocation/</url>
      
        <content type="html"><![CDATA[<p>前置知识：</p><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2>]]></content>
      
      
      
        <tags>
            
            <tag> Topic Modeling </tag>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Probabilistic Graphical Models</title>
      <link href="/blog/Probabilistic-Graphical-Models/"/>
      <url>/blog/Probabilistic-Graphical-Models/</url>
      
        <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>（施工中…）</p><h2 id="Bayesian-networks"><a href="#Bayesian-networks" class="headerlink" title="Bayesian networks"></a>Bayesian networks</h2><script type="math/tex; mode=display">p(x_1,x_2,\dots, x_n)=\prod_{i\in V}p(x_i\mid \boldsymbol{x}_{Pa(i)})</script><p>（施工中…）</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Gaussian Mixture Model</title>
      <link href="/blog/Gaussian-Mixture-Model/"/>
      <url>/blog/Gaussian-Mixture-Model/</url>
      
        <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>GMM算法是一种基于概率统计的聚类算法，其核心思想是将数据集中的数据看成是由多个高斯分布混合而成的。这些高斯分布的参数是未知的，需要通过EM算法来进行估计。具体而言，GMM算法首先随机初始化高斯分布的参数，然后通过EM算法进行迭代优化，直到收敛为止。在E步中，计算每个数据点属于每个高斯分布的概率；在M步中，根据这些概率更新每个高斯分布的参数。通过迭代优化，GMM算法可以自动地确定最优的高斯分布个数和参数，将数据集划分为不同的聚类簇。GMM算法的优点在于能够处理复杂的数据分布，且对噪声和异常点的影响相对较小。但是，GMM算法的计算量较大，且需要预先设定高斯分布的个数。</p><h2 id="GMM"><a href="#GMM" class="headerlink" title="GMM"></a>GMM</h2><p>（施工中…）</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Probabilistic Latent Semantic Analysis</title>
      <link href="/blog/Probabilistic-Latent-Semantic-Analysis/"/>
      <url>/blog/Probabilistic-Latent-Semantic-Analysis/</url>
      
        <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>概率潜在语义分析（Probabilistic Latent Semantic Analysis, PLSA）是一种用于文本处理、信息检索和主题分析的概率模型。其在潜在语义分析LSA的基础上，通过引入概率图模型为基础，对文档和主题进行建模。</p><p>它假设一篇文档中的每个词都是从一组概率分布中随机生成的，这组分布被称为“主题”。在PLSA模型中，每个文档都被表示为一组主题的概率分布，而每个主题也被表示为一组单词的概率分布。因此，PLSA模型可以将文档和单词映射到低维主题空间中，这有助于提取文本的隐藏语义信息。</p><p>PLSA模型的训练过程涉及到最大化似然函数，通常使用EM算法来实现。在使用PLSA进行文本分类、信息检索、主题建模等任务时，可以将文档表示为主题的概率分布，从而对文本进行分类、匹配等操作。</p><h2 id="LSA（潜在语义分析）"><a href="#LSA（潜在语义分析）" class="headerlink" title="LSA（潜在语义分析）"></a>LSA（潜在语义分析）</h2><p>潜在语义分析（Latent Semantic Analysis, LSA）基于词项在文档集合中的共现特性，表达词项的潜在意义。它通过“降维”的方式获得主题。</p><p>假设我们已经给出了一个文本文档集合$D = \{d_1,\dots, d_M\}$与词汇表$W = \{w_1，\dots, w_N\}$，其中$M$表示文档数，$N$表示词项数，对每篇文档的单词词频进行统计并建立词项文档矩阵（term-document matrix）$C\in \mathbb{R}^{N\times M}$。由于文档分布在稀疏词汇空间上，这种文档表示难以分析利用（即$C$往往是稀疏矩阵），因此通过降维让文档分布在一个维度有限的潜在语义空间上。LSA利用矩阵低秩近似方式将词项信息映射到潜在语义空间 (主题空间)。对$C$进行截断的奇异值分解（truncated SVD），截断所用的$K$则为主题数量。</p><script type="math/tex; mode=display">C=U\Sigma V^T,\quad C\approx \hat{U}\hat{\Sigma} \hat{V}^T (\text{truncated})</script><p>其中$\hat{U}\in \mathbb{R}^{N \times K}$是词项特征向量组成的矩阵，表示词项在隐含语义空间（主题空间）中的变换，$CC^T$表示词项相似度，值为两个词项在文档中共现次数；$\hat{V}\in \mathbb{R}^{M \times K}$是文档特征向量组成的矩阵，表示文档在隐含语义空间（主题空间）中的变换，$C^TC$表示文档相似度，值为两个文档中共同词项的数量。通过奇异值分解我们即完成了对词和文档的分类，同时得到了不同主题的文章其相关的高频词。</p><p>LSA的优势是可以一定程度建模一义多词现象，同时可以得到词项相似度、文档相似度、词项与文档的相似度，可用于信息检索。但其仍有缺陷，潜在语义空间中的元素缺少解释；$K$是经验性的设定；SVD求解复杂度较高，存在数值不稳定问题，当有新的文档来到时，若要更新模型需重新训练。</p><h2 id="PLSA（概率潜在语义分析）"><a href="#PLSA（概率潜在语义分析）" class="headerlink" title="PLSA（概率潜在语义分析）"></a>PLSA（概率潜在语义分析）</h2><p><b><span style="color:red;">前置知识：</span></b><a href="/blog/EM-algorithm/">EM算法</a>，<a href="/blog/Probabilistic-Graphical-Models/">概率图模型</a></p><p>PLSA加入了主题这一隐变量，记主题集合为$Z = \{z_1,\dots, z_K\}$。主题模型主要关心文档生成主题，主题生成词汇。PLSA利用概率方法学习隐含的主题分布，是一种基于有向图的生成模型：<br><img src="/blog/Probabilistic-Latent-Semantic-Analysis/1.png" alt="PLSA概率图模型" title="PLSA概率图模型"></p><p>上述变量有两个状态：observed ($d,w$), latent ($z$)。由概率图的定义，联合概率$p(d_i,w_j,z_k)=p(d_i)p(z_k\mid d_i)p(w_j\mid z_k)$。其中$p(d_i)$表示生成文档$d_i$的概率；$p(z_k\mid d_i)$表示文档$d_i$生成主题$z_k$的概率（每个文档$d_i$的主题概率分布）；$p(w_j\mid z_k)$表示主题$z_k$生成词项$w_j$的概率（每个主题$z_k$的词项概率分布）。</p><p>我们需要对观测到的变量$d, w$做最大似然估计，得到对数似然函数：</p><script type="math/tex; mode=display">\begin{align*}    L &=\log \left(\prod_{i=1}^M\prod_{j=1}^Np(d_i,w_j)^{n(d_i,w_j)}\right)\\\\    &=\sum_{i=1}^M\sum_{j=1}^Nn(d_i,w_j)\log p(d_i,w_j)\\\\    &=\sum_{i=1}^M\sum_{j=1}^Nn(d_i,w_j)\log \left(\sum_{k=1}^Kp(d_i,w_j,z_k)\right)\\\\    &=\sum_{i=1}^M\sum_{j=1}^Nn(d_i,w_j)\log \left(\sum_{k=1}^Kp(d_i)p(z_k\mid d_i)p(w_j\mid z_k)\right)\\\\    &\propto \sum_{i=1}^M\sum_{j=1}^Nn(d_i,w_j)\log \left(\sum_{k=1}^Kp(z_k\mid d_i)p(w_j\mid z_k)\right)\end{align*}</script><p>其中$n(d_i,w_j)$表示文档$d_i$中词项$w_j$出现的次数。</p><p>在已知数据的情况下，$p(d_i)=\frac{1}{M}$为一常数，因此我们要优化的参数即为$p(z_k\mid d_i), p(w_j\mid z_k)$，约有$(M+N)K$个。由于对数里求和的形式不容易直接优化，且含有隐变量，因此考虑使用EM算法求解。</p><p>在E-step中，我们假设模型参数给定，即$\hat{p}(z_k\mid d_i), \hat{p}(w_j\mid z_k)$已知，可求解后验概率$\hat{p}(z_k\mid d_i,w_j)$：</p><script type="math/tex; mode=display">\begin{align*}    \hat{p}(z_k\mid d_i,w_j)&=\frac{\hat{p}(d_i,w_j,z_k)}{\hat{p}(d_i,w_j)}\\\\    &=\frac{\hat{p}(z_k\mid d_i)\hat{p}(w_j\mid z_k)}{\sum_{k^{\prime}}^K \hat{p}(z_{k^{\prime}}\mid d_i)\hat{p}(w_j\mid z_{k^{\prime}})}\end{align*}</script><p>得到当前需优化的Q函数：</p><script type="math/tex; mode=display">Q=\sum_{i=1}^M\sum_{j=1}^Nn(d_i,w_j)\sum_{k=1}^K \hat{p}(z_k\mid d_i,w_j)\log \left(p(z_k\mid d_i)p(w_j\mid z_k)\right)</script><p>在M-step中，我们则需优化参数$p(z_k\mid d_i), p(w_j\mid z_k)$来最大化Q函数。这是个受限制的优化，需要各参数满足：</p><script type="math/tex; mode=display">\begin{gather*}     \sum_{k=1}^Kp(z_k\mid d_i)=1,\quad i=1,2,\dots,M\\     \sum_{j=1}^Np(w_j\mid z_k)=1,\quad k=1,2,\dots,K\end{gather*}</script><p>因此引入拉格朗日乘子$\boldsymbol{\lambda}=\{\lambda_1,\dots,\lambda_M\}, \boldsymbol{\mu}=\{\mu_1,\dots,\mu_K\}$，将优化目标修改为:</p><script type="math/tex; mode=display">\mathcal{H}=Q+\sum_{i=1}^M\lambda_i\left(1-\sum_{k=1}^Kp(z_k\mid d_i)\right)+\sum_{k=1}^K\mu_k\left(1-\sum_{j=1}^Np(w_j\mid z_k)\right)</script><p>对各参数求导并令导数为0，得到:</p><script type="math/tex; mode=display">\begin{gather*}    \sum_{j=1}^{N}n(d_i,w_j)\hat{p}(z_k\mid d_i,w_j)-\lambda_i p(z_k\mid d_i)=0, \quad \forall\, k,i\\\\    \sum_{i=1}^Mn(d_i,w_j)\hat{p}(z_k\mid d_i,w_j)-\mu_k p(w_j\mid z_k)=0,\quad \forall\, j,k\end{gather*}</script><p>利用概率归一化条件，上述两个式子分别对$k,j$求和，得到：</p><script type="math/tex; mode=display">\begin{gather*}\lambda_i=\sum_{k=1}^K\sum_{j=1}^{N}n(d_i,w_j)\hat{p}(z_k\mid d_i,w_j),\quad \forall \,i\\\\\mu_k=\sum_{j=1}^N\sum_{i=1}^Mn(d_i,w_j)\hat{p}(z_k\mid d_i,w_j),\quad \forall \, k\end{gather*}</script><p>整理可得极大化Q函数时各个模型参数的值：</p><script type="math/tex; mode=display">\begin{align*}    p(z_k\mid d_i)&=\frac{\sum_{j=1}^{N}n(d_i,w_j)\hat{p}(z_k\mid d_i,w_j)}{\sum_{k^{\prime}=1}^K\sum_{j=1}^{N}n(d_i,w_j)\hat{p}(z_{k^{\prime}}\mid d_i,w_j)}\\\\    &=\frac{\sum_{j=1}^{N}n(d_i,w_j)\hat{p}(z_k\mid d_i,w_j)}{n(d_i)}\\\\    p(w_j\mid z_k)&=\frac{\sum_{i=1}^Mn(d_i,w_j)\hat{p}(z_k\mid d_i,w_j)}{\sum_{j^{\prime}=1}^N\sum_{i=1}^Mn(d_i,w_{j^{\prime}})\hat{p}(z_k\mid d_i,w_{j^{\prime}})}\end{align*}</script><p>其中$n(d_i)=\sum_{j=1}^Nn(d_i,w_j)$，表示文档$d_i$中的词项总数。</p><p>总结PLSA的流程：</p><ul><li>设置参数$p(z_k\mid d_i), p(w_j\mid z_k)$的初始值；</li><li>迭代执行E-step, M-step直至收敛或达到最大迭代轮次，得到参数$p(z_k\mid d_i), p(w_j\mid z_k)$的终值。</li></ul><p>PLSA是LSA的概率化版本，主题以词的概率分布表示，文档则以主题的概率分布表示，相比LSA，概率化的理论解释性更好。但主题数量仍然需要经验性确定；需要学习的参数量多，有过拟合的可能性，需要大量的样本才能得到较好的模型；没有解决为未知文档分配主题概率的问题。</p><p><strong>Reference</strong></p><p>上课的PPT</p><p><a href="https://link.springer.com/article/10.1023/A:1007617005950">Unsupervised Learning by Probabilistic Latent Semantic Analysis | SpringerLink</a></p><p><a href="https://zhuanlan.zhihu.com/p/401513907">【学习笔记】pLSA算法原理手写推导 - 知乎 (zhihu.com)</a></p><p><a href="https://zhuanlan.zhihu.com/p/31235789">pLSA原理及其代码实现 - 知乎 (zhihu.com)</a></p><p><a href="https://zhuanlan.zhihu.com/p/54168587">情感分析中的PLSA、LDA模型 - 知乎 (zhihu.com)</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> EM algorithm </tag>
            
            <tag> Topic Modeling </tag>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>EM algorithm</title>
      <link href="/blog/EM-algorithm/"/>
      <url>/blog/EM-algorithm/</url>
      
        <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>期望最大化算法（expectation-maximization algorithm, EM algorithm）是最常见的隐变量估计方法，目的是求解带有隐变量的最大似然问题。EM算法是一种迭代优化策略，它的计算方法中每一次迭代都分两步，其中一个为期望步（E步），另一个为极大步（M步）。在E步中，算法假设已经有了一组参数估计，然后根据这组参数估计，计算出隐含变量的期望值。在M步中，算法根据E步计算出的隐含变量的期望值，通过最大化对数似然函数的方式来更新参数估计。通过交替执行E步和M步，EM算法可以逐步优化模型参数，直到达到收敛条件为止。虽然EM算法不一定能够得到全局最优解，但是在许多实际问题中，它通常能够得到较好的局部最优解。</p><h2 id="EM算法推导"><a href="#EM算法推导" class="headerlink" title="EM算法推导"></a>EM算法推导</h2><p>我们记$\boldsymbol{X}$为所有观测到的数据，$\boldsymbol{Z}$为所有的隐变量，其对数似然函数为：</p><script type="math/tex; mode=display">% \begin{equation}    L(\theta)=\log p(\boldsymbol{X};\theta) =\log \sum_{z \in \boldsymbol{Z}} p(\boldsymbol{X}, z;\theta)% \end{equation}</script><p>我们的目的是求出各参数让似然函数值达到最大，但上式中存在对数里求和的形式不方便求解，我们引入一个新的与隐变量有关的分布$q(z)$， $\sum_{z\in \boldsymbol{Z} }q(z)=1$：</p><script type="math/tex; mode=display">% \begin{equation}    L(\theta)=\log \sum_{z \in \boldsymbol{Z}} q(z)\frac{p(\boldsymbol{X}, z;\theta)}{q(z)}\geq \sum_{z \in \boldsymbol{Z}} q(z) \log \frac{p(\boldsymbol{X}, z;\theta)}{q(z)}% \end{equation}</script><p>上述不等式可由Jensen不等式导出，当且仅当存在一个常数$c$，使得对$\forall z \in Z$，有：</p><script type="math/tex; mode=display">\frac{p(X,z;\theta)}{q(z)}=c</script><p>由</p><script type="math/tex; mode=display">\sum_{z\in \boldsymbol{Z}} q(z)=\frac{1}{c}\sum_{z\in \boldsymbol{Z}}p(\boldsymbol{X},z;\theta)=\frac{1}{c}p(\boldsymbol{X};\theta)=1</script><p>得$\forall z \in Z$:</p><script type="math/tex; mode=display">q(z)=\frac{p(X,z;\theta)}{c}=\frac{p(X,z;\theta)}{p(\boldsymbol{X};\theta)}=p(z\mid \boldsymbol{X};\theta)</script><p>由此我们可以得到$L(\theta)$的一个下界：</p><script type="math/tex; mode=display">\sum_{z \in \boldsymbol{Z}} p(z\mid \boldsymbol{X};\theta) \log \frac{p(\boldsymbol{X}, z;\theta)}{p(z\mid \boldsymbol{X};\theta)}=E_{\boldsymbol{Z}|\boldsymbol{X};\theta}[\log p(\boldsymbol{X}, z;\theta)]+H(p(\boldsymbol{Z}|\boldsymbol{X};\theta))</script><p>如果我们能极大化这个下界，则也在尝试极大化我们的对数似然。若给定模型参数$\theta=\hat{\theta}$，则后验概率$p(z|\boldsymbol{X};\hat{\theta})$可求，$H(p(\boldsymbol{Z}|\boldsymbol{X};\hat{\theta}))$为一常数，此时我们只需要极大化条件期望$E_{\boldsymbol{Z}|\boldsymbol{X};\hat{\theta}}[\log p(\boldsymbol{X}, z;\theta)]$，更新$\theta$。通过交替更新的思路，不断迭代重复上述过程，以此来优化这个问题。</p><h2 id="EM算法流程"><a href="#EM算法流程" class="headerlink" title="EM算法流程"></a>EM算法流程</h2><ul><li>随机初始化模型参数$\theta$的初值$\theta^{(0)}$</li><li>迭代轮数$t$小于等于最大迭代轮数$T$时：<ul><li>E-step: 求解后验概率$p(z|\boldsymbol{X};\theta^{t})$，并得到当前需优化期望表达式（Q函数）<script type="math/tex">Q(\theta;\theta^{(t)})=E_{\boldsymbol{Z}|\boldsymbol{X};\theta^{t}}[\log p(\boldsymbol{X}, z;\theta)]</script></li><li>M-step: 极大化$Q(\theta;\theta^{(t)})$，得到$\theta^{t+1}<script type="math/tex">$\theta^{t+1}=\mathop{\arg\max}\limits_{\theta}  Q(\theta;\theta^{(t)})</script></li><li>判断$\theta^{t+1}$是否收敛，若收敛则终止算法，否则继续交替地进行两个步骤。</li></ul></li></ul><h2 id="EM算法收敛性证明"><a href="#EM算法收敛性证明" class="headerlink" title="EM算法收敛性证明"></a>EM算法收敛性证明</h2><p>（施工中…）</p><p><strong>Reference</strong></p><p><a href="https://www.cnblogs.com/pinard/p/6912636.html">EM算法原理总结 - 刘建平Pinard - 博客园 (cnblogs.com)</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> EM algorithm </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
